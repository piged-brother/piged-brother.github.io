<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>吴恩达深度学习课程第一课</title>
      <link href="/2023/10/11/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%B8%80%E8%AF%BE/"/>
      <url>/2023/10/11/%E5%90%B4%E6%81%A9%E8%BE%BE%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%B8%80%E8%AF%BE/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><p><img src="C:\Users\阿千\Desktop\深度学习笔记\监督学习流程.png" alt="监督学习流程"> </p><h1 id="二元分类"><a href="#二元分类" class="headerlink" title="二元分类"></a><strong>二元分类</strong></h1><p>意思就是是猫或者不是猫 这里只是打个比方。</p><h3 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h3><p>常见的logistic回归就是sigmoid函数套住y=wx+b    公式如下</p><p>$$y=\vartheta (\mathbf{W}^\mathrm{T}x+b)$</p><p>使得其概率鉴于0-1之间使得sigmoid函数变成这样</p><p>$\vartheta (\hat{z} ) = \frac{1} { 1+e^{\hat{z}} }$</p><h3 id="损失函数：是衡量单一训练样例的效果"><a href="#损失函数：是衡量单一训练样例的效果" class="headerlink" title="损失函数：是衡量单一训练样例的效果"></a>损失函数：是衡量单一训练样例的效果</h3><p>损失函数定义位$\delta (\hat{y} ,y)=\frac{1}{z} (\hat{y}-y)^2$  就是定义$\hat{y}$和y之间的距离有多近 这里的$\hat{y}$ 就代表了sigmoid(z)函数</p><p>但是这个损失函数更好因为e为底 $\delta =-(y(log \hat{y}+(1-y)log(1- \hat{y}))$</p><p>损失函数有负号的原因是在逻辑回归中我们需要最小化损失函数，但是由于有个负号并且log函数的单调递增的，  导致其最小化的损失函数就是最大化的logP（y|X）</p><p>损失函数又叫做误差函数，用来衡量算法的运行情况，Loss function: L ( $\hat{y}$ , y ) </p><p>我们通过这个称为 L LL 的损失函数，来衡量预测输出值和实际值有多接近。一般我们用预测值和实际值的平方差或者它们平方差的一半，但是通常在逻辑回归中我们不这么做，因为当我们在学习逻辑回归参数的时候，会发现我们的优化目标不是凸优化，只能找到多个局部最优值，梯度下降法很可能找不到全局最优值，虽然平方差是一个不错的损失函数，但是我们在逻辑回归模型中会定义另外一个损失函数。</p><p>我们在逻辑回归中用到的损失函数是：<br>$ J=-(y(log \hat{y}+(1-y)log(1- \hat{y}))$</p><h3 id="成本函数-衡量w和b的效果"><a href="#成本函数-衡量w和b的效果" class="headerlink" title="成本函数:衡量w和b的效果"></a>成本函数:衡量w和b的效果</h3><p>成本函数是  $J(\delta,b)=\frac{1}{m} \sum_{1}^{m} \delta (\hat{y} ,y) = -\frac{1}{m} \sum_{1}^{m} [(-y^{i}(log\hat{y}^{i}+(1-y^{i})log(1- \hat{y}^{i} ))]$ y代表1或0 举个是否鉴别猫的例子</p><p>1/m是可以加或者不加的 但是加的原因是对可以对单个成本进行估计，也就是适当的缩放损失函数。</p><h3 id="梯度下降法：找到最小的成本函数"><a href="#梯度下降法：找到最小的成本函数" class="headerlink" title="梯度下降法：找到最小的成本函数"></a>梯度下降法：找到最小的成本函数</h3><p><img src="C:\Users\阿千\Desktop\深度学习笔记\1694484928365.jpg" alt="1694484928365"></p><p>找到最低点 令$w=w-\alpha \frac{\mathrm{d} j(w)}{\mathrm{d} w} $</p><p>找到极小值  意思就是牛顿迭代， 举个例子w在极小值（最低点）右边  此时斜率&gt;0 但是$\alpha \frac{\mathrm{d} j(w)}{\mathrm{d} w}$&gt;0</p><p>这时候w就会变小  左边就同理</p><p>同理 找成本函数的参数b的时候<strong>$b=b-\alpha \frac{\mathrm{d} j(b)}{\mathrm{d} b} $</strong></p><h4 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h4><p>  <img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230912104555026.png" alt="image-20230912104555026">a</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230912150316908.png" alt="image-20230912150316908"></p><h2 id="向量化：加速计算，摆脱显示for循环语句"><a href="#向量化：加速计算，摆脱显示for循环语句" class="headerlink" title="向量化：加速计算，摆脱显示for循环语句"></a>向量化：加速计算，摆脱显示for循环语句</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">import time//此案例为比较向量化运算和非向量化运算之间的速度</span><br><span class="line">a = np.random.rand(1000000)</span><br><span class="line">b = np.random.rand(1000000)</span><br><span class="line"></span><br><span class="line">tic = time.time()</span><br><span class="line">c = np.dot(a,b)</span><br><span class="line">toc = time .time()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">print(c)</span><br><span class="line">print(&quot;vector :&quot; + str(1000*(toc-tic)) + &quot;ms&quot;)</span><br><span class="line"></span><br><span class="line">c=0;</span><br><span class="line">tic = time.time()</span><br><span class="line"></span><br><span class="line">for i in range(1000000):</span><br><span class="line">    c+= a[i]*b[i]</span><br><span class="line">toc = time.time()</span><br><span class="line">print(c)</span><br><span class="line">print(&quot;for loop&quot;+ str(1000*(toc-tic)) + &quot;ms&quot;)</span><br></pre></td></tr></table></figure><h3 id="向量化的logistic回归"><a href="#向量化的logistic回归" class="headerlink" title="向量化的logistic回归"></a>向量化的logistic回归</h3><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230912170514411.png" alt="image-20230912170514411"></p><p>将z写成向量乘法的形式用np内置库进行运算从而摆脱for循环</p><p>python<strong>的广播机制</strong>：意思就是将一个向量+上一个常数后 该常熟就自动被扩展成与该向量行和列匹配的向量</p><p>如</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230912171046265.png" alt="image-20230912171046265"></p><p>等于</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230912171059528.png" alt="image-20230912171059528"></p><p>dz 和dw的计算脱离for循环变成</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230912172407404.png" alt="image-20230912172407404"></p><p>非向量化的 logic回归版本：<img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230912172526590.png" alt="image-20230912172526590"></p><p>向量化后简化的版本：</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230912173545392.png" alt="image-20230912173545392"></p><p>ps: python 广播机制举例：3x4矩阵+1x4矩阵   1x4矩阵会自动扩充成 3x4矩阵   </p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230913152604028.png" alt="image-20230913152604028"></p><p>注：在使用数据的时候尽量不要使用矩阵秩为1的矩阵来操作</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">#实现sigmoid函数</span><br><span class="line"></span><br><span class="line">def sigmoid(x):</span><br><span class="line">   s= 1 /(np.exp(-x) +1)#np.exp（x）适用于任何np.array x并将指数函数应用于每个坐标</span><br><span class="line">    return s</span><br><span class="line">def sigmoid_derivative(x):</span><br><span class="line">    s= sigmoid(x)</span><br><span class="line">    ds = s*(1-s)</span><br><span class="line">    return ds</span><br><span class="line"></span><br><span class="line">#实现输入采用维度为(length, height, 3)的输入，并返回维度为(length*height*3, 1)的向量。</span><br><span class="line">def image2vector(image):</span><br><span class="line">    v = image.reshape((image.shape[0]*image.shape[1]*image.shape[2]),1)</span><br><span class="line">    return v</span><br><span class="line"></span><br><span class="line">image = np.array([[[ 0.67826139,  0.29380381],</span><br><span class="line">        [ 0.90714982,  0.52835647],</span><br><span class="line">        [ 0.4215251 ,  0.45017551]],</span><br><span class="line">         [[ 0.92814219,  0.96677647],</span><br><span class="line">    [ 0.85304703,  0.52351845],</span><br><span class="line">    [ 0.19981397,  0.27417313]],</span><br><span class="line"></span><br><span class="line">   [[ 0.60659855,  0.00533165],</span><br><span class="line">    [ 0.10820313,  0.49978937],</span><br><span class="line">    [ 0.34144279,  0.94630077]]])</span><br><span class="line">    print (&quot;image2vector(image) = &quot; + str(image2vector(image)))</span><br></pre></td></tr></table></figure><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">x1 = [9, 2, 5, 0, 0, 7, 5, 0, 0, 0, 9, 2, 5, 0, 0]</span><br><span class="line">x2 = [9, 2, 2, 9, 0, 9, 2, 5, 0, 0, 9, 2, 5, 0, 0]</span><br><span class="line"></span><br><span class="line">### VECTORIZED DOT PRODUCT OF VECTORS ###</span><br><span class="line">tic = time.process_time()</span><br><span class="line">dot = np.dot(x1,x2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line">print (&quot;dot = &quot; + str(dot) + &quot;\n ----- Computation time = &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line"></span><br><span class="line">### VECTORIZED OUTER PRODUCT ###</span><br><span class="line">tic = time.process_time()</span><br><span class="line">outer = np.outer(x1,x2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line">print (&quot;outer = &quot; + str(outer) + &quot;\n ----- Computation time = &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line"></span><br><span class="line">### VECTORIZED ELEMENTWISE MULTIPLICATION ###</span><br><span class="line">tic = time.process_time()</span><br><span class="line">mul = np.multiply(x1,x2)</span><br><span class="line">toc = time.process_time()</span><br><span class="line">print (&quot;elementwise multiplication = &quot; + str(mul) + &quot;\n ----- Computation time = &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line"></span><br><span class="line">### VECTORIZED GENERAL DOT PRODUCT ###</span><br><span class="line">tic = time.process_time()</span><br><span class="line">dot = np.dot(W,x1)</span><br><span class="line">toc = time.process_time()</span><br><span class="line">print (&quot;gdot = &quot; + str(dot) + &quot;\n ----- Computation time = &quot; + str(1000*(toc - tic)) + &quot;ms&quot;)</span><br><span class="line">dot = 278</span><br><span class="line"> ----- Computation time = 0.0ms</span><br><span class="line">outer = [[81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]</span><br><span class="line"> [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]</span><br><span class="line"> [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]</span><br><span class="line"> [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]</span><br><span class="line"> [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]</span><br><span class="line"> [63 14 14 63  0 63 14 35  0  0 63 14 35  0  0]</span><br><span class="line"> [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]</span><br><span class="line"> [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]</span><br><span class="line"> [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]</span><br><span class="line"> [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]</span><br><span class="line"> [81 18 18 81  0 81 18 45  0  0 81 18 45  0  0]</span><br><span class="line"> [18  4  4 18  0 18  4 10  0  0 18  4 10  0  0]</span><br><span class="line"> [45 10 10 45  0 45 10 25  0  0 45 10 25  0  0]</span><br><span class="line"> [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]</span><br><span class="line"> [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]]</span><br><span class="line"> ----- Computation time = 0.0ms</span><br><span class="line">elementwise multiplication = [81  4 10  0  0 63 10  0  0  0 81  4 25  0  0]</span><br><span class="line"> ----- Computation time = 0.0ms</span><br><span class="line">gdot = [ 21.57937154  22.58814194  13.70092277]</span><br><span class="line"> ----- Computation time = 0.0ms</span><br></pre></td></tr></table></figure><p>总的来说还是根据sgmoid公式来进行概率的预测</p><p>先初始化w和b  然后通过梯度下降来优化获得更好的w和b</p><p>然后通过预测函数来进行概率预测</p><h3 id="总结逻辑回归需要先算"><a href="#总结逻辑回归需要先算" class="headerlink" title="总结逻辑回归需要先算"></a>总结逻辑回归需要先算</h3><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917193629126.png" alt="image-20230917193629126"></p><p>然后算 $\vartheta (\hat{z} )$ 也就是sigmoid(z)</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917103418475.png" alt="image-20230917103418475"></p><h1 id="什么是神经网络"><a href="#什么是神经网络" class="headerlink" title="什么是神经网络"></a>什么是神经网络</h1><p>如上图，类比与监督学习的逻辑回归来说  神经网络就是多个</p><p>逻辑回归叠加的结果，叠加后到达下一层。</p><p><img src="https://img-blog.csdnimg.cn/20200415163553547.png" alt="img"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917105353093.png" alt="image-20230917105353093"></p><p>首先你需要输入特征 x ，参数 w 和b ，通过这些你就可以计算出  z ，</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917105433820.png" alt="image-20230917105433820"></p><p>接下来使用就可以计算出 a aa 。我们将的符号换为表示输出 y ^ → a = σ ( z ) \hat{y}\rightarrow a=\sigma(z)<br>y<br>^</p><p> →a=σ(z) ,然后可以计算出loss function L ( a , y ) L(a,y)L(a,y)</p><p>神经网络看起来是如下这个样子。正如我之前已经提到过，你可以把许多sigmoid单元堆叠起来形成一个神经网络。对于图3.1.1中的节点，它包含了之前讲的计算的两个步骤：首先通过公式计算出值z ，然后通过 σ ( z ) \sigma(z)σ(z) 计算值 a 。</p><p><img src="https://img-blog.csdnimg.cn/20200415163612515.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>这个神经网络对于三个节点  首先计算第一层网络中的各个相关的数$z^{[1]} $</p><p>，接着计算$a^{[1]}$接着计算下一层网络 等等  意思就是不同层使用$^{[m]}$代表第m层中节点相关的数，这些节点的集合被称为第m层网络。</p><p>这样可以保证$^{[m]}$不会和我们之前标识单个训练样本的$^{(i)}$混淆 公式3.3如下</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917110321224.png" alt="image-20230917110321224"></p><p>类似逻辑回归，在计算后需要使用计算，接下来你需要使用另外一个线性方程对应的参数计算 $z^{[2]}$然后计算$a^{[2]}$</p><p>  ，此时 $a^{[2]}$ 就是整个神经网络最终的输出，用$\hat{y}$ </p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917110729088.png" alt="image-20230917110729088"></p><h3 id=""><a href="#" class="headerlink" title=" "></a> </h3><h1 id="神经网络表示"><a href="#神经网络表示" class="headerlink" title="神经网络表示"></a>神经网络表示</h1><p><img src="https://img-blog.csdnimg.cn/20200416173622491.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>我们有输入特征 x 1 、 x 2 、 x 3 x_1、x_2、x_3x<br>1</p><p> 、x<br>2</p><p> 、x<br>3</p><p>  ，它们被竖直地堆叠起来，这叫做神经网络的输入层(Input Layer)。它包含了神经网络的输入；然后这里有另外一层我们称之为隐藏层(Hidden Layer)（图3.2.1的四个结点）。待会儿我会回过头来讲解术语”隐藏”的意义；在本例中最后一层只由一个结点构成，而这个只有一个结点的层被称为输出层(Output Layer)，它负责产生预测值。解释隐藏层的含义：在一个神经网络中，当你使用监督学习训练它的时候，训练集包含了输入 x xx 也包含了目标输出 y yy ，所以术语隐藏层的含义是在训练集中，这些中间结点的准确值我们是不知道到的，也就是说你看不见它们在训练集中应具有的值。你能看见输入的值，你也能看见输出的值，但是隐藏层中的东西，在训练集中你是无法看到的。所以这也解释了词语隐藏层，只是表示你无法在训练集中看到他们。</p><p>现在讲层数 </p><p>就像我们之前用向量 x  表示输入特征。这里有个可代替的记号 $ a^{[0]}$可以用来表示输入特征。 a  表示激活的意思，它意味着网络中不同层的值会传递到它们后面的层中，输入层将 x 传递给隐藏层，所以我们将输入层的激活值称为 $a^{[0]}$ 下一层即隐藏层也同样会产生一些激活值，那么我将其记作 $a^{[1]}$ ，所以具体地，这里的第一个单元或结点我们将其表示为 $a_{1}^{[1]}$   ，第二个结点的值我们记为 $ a_{2}^{[1]}$ 以此类推。所以这里的是一个四维的向量如果写成Python代码，那么它是一个规模为4x1的矩阵或一个大小为4的列向量，如下公式，它是四维的，因为在本例中，我们有四个结点或者单元，或者称为四个隐藏层单元</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917112841893.png" alt="image-20230917112841893"></p><p>最后输出层将产生某个数值 a  ，它只是一个单独的实数，所以 $ \hat{y}$ 的值将取为 $a^{[2]}$  。这与逻辑回归很相似，在逻辑回归中，我们有 y$^ \hat{y}$   直接等于 a  ，在逻辑回归中我们只有一个输出层，所以我们没有用带方括号的上标。但是在神经网络中，我们将使用这种带上标的形式来明确地指出这些值来自于哪一层，有趣的是在约定俗成的符号传统中，在这里你所看到的这个例子，只能叫做一个两层的神经网络（如下图）。原因是当我们计算网络的层数时，输入层是不算入总层数内，所以隐藏层是第一层，输出层是第二层。第二个惯例是我们将输入层称为第零层，所以在技术上，这仍然是一个三层的神经网络，因为这里有输入层、隐藏层，还有输出层。但是在传统的符号使用中，如果你阅读研究论文或者在这门课中，你会看到人们将这个神经网络称为一个两层的神经网络，因为我们不将输入层看作一个标准的层。同理w和b加上上标也是对应不同层的不同参数类似$ w^{[1]}$ 和$b^{[1]}$</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917113049572.png" alt="image-20230917113049572"></p><p>接下来是对各个层的神经单元进行的公式举例如图<img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917191923227.png" alt="image-20230917191923227"></p><p>当然这个用四个for循环效率太低   可以使之向量化从而减轻运算量</p><p>如</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917192457917.png" alt="image-20230917192457917"></p><p>上面相乘是 4x3矩阵 乘以 3x1的矩阵在加一个4x1的矩阵从而得到$z^{[1]}$</p><p> w一开始是3x4的</p><p>接下来是向量化</p><p>注意$a^{<a href="i">1</a>}$ 这里的i标识第几个样本 1标识第一层神经网路</p><p>m个样本要计算实现这四个公式</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917195503865.png" alt="image-20230917195503865"></p><p>接下来就是将向量横向堆积的过程<img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917201131159.png" alt="image-20230917201131159"></p><p>这里具体就是将${z^{<a href="1">1</a>},z^{<a href="2">1</a>}…….z^{<a href="m">1</a>}}$进行堆积形成$Z^{[1]}$然后通过$\vartheta (z^{<a href="1">1</a>}),\vartheta (z^{<a href="2">1</a>})…….\vartheta (z^{<a href="m">1</a>})$向量横向排列形成$A^{[a]}$</p><p>像这样</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917201739141.png" alt="image-20230917201739141"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917201941190.png" alt="image-20230917201941190"></p><p>经过此处导可以得出$z^{[1]} = w^{[1]}X + b^{[1]}$</p><p>如下图  </p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917202420741.png" alt="image-20230917202420741"></p><h3 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h3><p>这里有sigmoid函数和tanh函数<img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917202959386.png" alt="image-20230917202959386"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917203115556.png" alt="image-20230917203115556"></p><p>其实tanh函数是sigmoid函数平移的结果</p><p>tanh函数更好用 </p><p>sigmoid函数和tanh函数两者共同的缺点是，在 z 特别大或者特别小的情况下，导数的梯度或者函数的斜率会变得特别小，最后就会接近于0，导致降低梯度下降的速度。</p><p>机械学习中还有这个函数ReLu修正线性单元</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230917203802613.png" alt="image-20230917203802613"></p><p>选择激活函数有技巧：</p><p>如果输出是0、1值（二分类问题），则输出层选择sigmoid函数，然后其它的所有单元都选择Relu函数。</p><p>这是很多激活函数的默认选择，如果在隐藏层上不确定使用哪个激活函数，那么通常会使用Relu激活函数。有时，也会使用tanh激活函数，但Relu的一个优点是：当 z zz 是负值的时候，导数等于0。</p><p>这里也有另一个版本的Relu被称为Leaky Relu。</p><p>当 z zz 是负值时，这个函数的值不是等于0，而是轻微的倾斜，如图。</p><p>这个函数通常比Relu激活函数效果要好，尽管在实际中Leaky ReLu使用的并不多。</p><p><img src="https://img-blog.csdnimg.cn/20200416195840688.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p><h3 id="为什么要用非线性激活函数？"><a href="#为什么要用非线性激活函数？" class="headerlink" title="为什么要用非线性激活函数？"></a>为什么要用非线性激活函数？</h3><p>实证明，如果你使用线性激活函数或者没有使用一个激活函数，那么无论你的神经网络有多少层一直在做的只是计算线性函数，所以不如直接去掉全部隐藏层。在我们的简明案例中，事实证明如果你在隐藏层用线性激活函数，在输出层用sigmoid函数，那么这个模型的复杂度和没有任何隐藏层的标准Logistic回归是一样的，如果你愿意的话，可以证明一下。</p><p>在这里线性隐层一点用也没有，因为这两个线性函数的组合本身就是线性函数，所以除非你引入非线性，否则你无法计算更有趣的函数，即使你的网络层数再多也不行；只有一个地方可以使用线性激活函数——— g ( z ) = z ，就是你在做机器学习中的回归问题。 y yy 是一个实数，举个例子，比如你想预测房地产价格， y  就不是二分类任务0或1，而是一个实数，从0到正无穷。如果 y y是个实数，那么在输出层用线性激活函数也许可行，你的输出也是一个实数，从负无穷到正无穷。</p><p>总而言之，不能在隐藏层用线性激活函数，可以用ReLU或者tanh或者leaky ReLU或者其他的非线性激活函数，唯一可以用线性激活函数的通常就是输出层；除了这种情况，会在隐层用线性函数的，除了一些特殊情况，比如与压缩有关的，那方面在这里将不深入讨论。在这之外，在隐层使用线性激活函数非常少见。因为房价都是非负数，所以我们也可以在输出层使用ReLU函数这样你的 $\hat{y}$ 都大于等于0。</p><h3 id="正向传播和反向传播的区别3-9-神经网络的梯度下降法-深度学习-Stanford吴恩达教授-吴恩达梯度下降法的公式-Zhao-Jichao的博客-CSDN博客"><a href="#正向传播和反向传播的区别3-9-神经网络的梯度下降法-深度学习-Stanford吴恩达教授-吴恩达梯度下降法的公式-Zhao-Jichao的博客-CSDN博客" class="headerlink" title="正向传播和反向传播的区别3.9 神经网络的梯度下降法-深度学习-Stanford吴恩达教授_吴恩达梯度下降法的公式_Zhao-Jichao的博客-CSDN博客"></a>正向传播和反向传播的区别<a href="https://blog.csdn.net/weixin_36815313/article/details/105340791">3.9 神经网络的梯度下降法-深度学习-Stanford吴恩达教授_吴恩达梯度下降法的公式_Zhao-Jichao的博客-CSDN博客</a></h3><p>为什么要反向传播？  </p><h4 id="反向传播就是为了实现最优化，省去了重复的求导步骤"><a href="#反向传播就是为了实现最优化，省去了重复的求导步骤" class="headerlink" title="反向传播就是为了实现最优化，省去了重复的求导步骤"></a>反向传播就是为了实现最优化，省去了重复的求导步骤</h4><p>这里不详细讨论方向传播算法的原理了，简单来说这种方法利用了函数求导的链式法则，从输出层到输入层逐层计算模型参数的梯度值，只要模型中每个计算都能求导，那么这种方法就没问题。可以看到按照这个方向计算梯度，各个神经单元只计算了一次，没有重复计算。这个计算方向能够高效的根本原因是，在计算梯度时前面的单元是依赖后面的单元的计算，而“从后向前”的计算顺序正好“解耦”了这种依赖关系，先算后面的单元，并且记住后面单元的梯度值，计算前面单元之就能充分利用已经计算出来的结果，避免了重复计算。</p><p><a href="https://blog.csdn.net/johnny_love_1968/article/details/117598649?ops_request_misc=%7B%22request%5Fid%22%3A%22169495764016800227457180%22%2C%22scm%22%3A%2220140713.130102334..%22%7D&amp;request_id=169495764016800227457180&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-117598649-null-null.142^v94^insert_down1&amp;utm_term=反向传播原理&amp;spm=1018.2226.3001.4187">深度学习——反向传播（Backpropagation）_南方惆怅客的博客-CSDN博客</a></p><p>首先，你需要正向传播，来计算z对w的偏导，进而求出sigmoid’(z)是多少。然后，根据输出层输出的数据进行反向传播，计算出l对z的偏导是多少，最后，代入到公式0当中，即可求出l对w的偏导是多少。注意，这个偏导，其实反应的就是梯度。然后我们利用梯度下降等方法，对这个w不断进行迭代（也就是权值优化的过程），使得损失函数越来越小，整体模型也越来越接近于真实值。</p><p><a href="https://blog.csdn.net/weixin_36815313/article/details/105341107">3.10 直观理解反向传播-深度学习-Stanford吴恩达教授_Zhao-Jichao的博客-CSDN博客</a></p><h3 id="随机初始化"><a href="#随机初始化" class="headerlink" title="随机初始化"></a>随机初始化</h3><p>一句话就是初始化不要全部为0，要随机初始化</p><p>如果 w  很大，那么你很可能最终停在（甚至在训练刚刚开始的时候） z z很大的值，这会造成tanh/Sigmoid激活函数饱和在龟速的学习上，如果你没有sigmoid/tanh激活函数在你整个的神经网络里，就不成问题。但如果你做二分类并且你的输出单元是Sigmoid函数，那么你不会想让初始参数太大，因此这就是为什么乘上0.01或者其他一些小数是合理的尝试。对于 $ w^{[2]}$   一样，就是np.random.randn((1,2))，我猜会是乘以0.01。</p><p>事实上有时有比0.01更好的常数，当你训练一个只有一层隐藏层的网络时（这是相对浅的神经网络，没有太多的隐藏层），设为0.01可能也可以。但当你训练一个非常非常深的神经网络，你可能要试试0.01以外的常数。下一节课我们会讨论怎么并且何时去选择一个不同于0.01的常数，但是无论如何它通常都会是个相对小的数。</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230918105011317.png" alt="image-20230918105011317"></p><p>就比如这个图 假如给的w参数很大  那么生成的z就很大  z 很大就会导致激活函数处在末端  也就是图中箭头位置 </p><p>那么从图中看出该出斜率比较平缓，从而导致计算反向梯度的时候会很慢。</p><h1 id="-1"><a href="#-1" class="headerlink" title=" "></a> </h1><h1 id="正向传播-反向传播"><a href="#正向传播-反向传播" class="headerlink" title="正向传播  反向传播"></a>正向传播  反向传播</h1><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230919170349561.png" alt="image-20230919170349561"></p><p>向量化版本</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230919170642156.png" alt="image-20230919170642156"></p><p>意思就是i先算出$da^{[l]}$</p><p>然后得出$da^{[l-1]}$,$dw^{[l]}$,$db^{[l]}$</p><p>深度神经网络学习中 正向传播可能不同隐藏层有着不同的激活函数  然后得到损失函数后 在进行迭代 反向传播  </p><p>如下图</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230919171014362.png" alt="image-20230919171014362"></p><h2 id="计算正向传播的矩阵维度会有一个规律"><a href="#计算正向传播的矩阵维度会有一个规律" class="headerlink" title="计算正向传播的矩阵维度会有一个规律"></a>计算正向传播的矩阵维度会有一个规律</h2><p>核对矩阵维数</p><p>w 的维度是（下一层的维数，前一层的维数），即 $w^{[l]}:(n^{[l]},n^{[l-1]})$</p><p>b 的维度是（下一层的维数，1） $b^{[l]}:(n^{[l]},1)$</p><p>同理  $dw^{[l]}:(n^{[l]},n^{[l-1]})$</p><p> $db^{[l]}:(n^{[l]},1)$</p><p><img src="https://img-blog.csdnimg.cn/20200407170126326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>单个数据样本$W^{[l]}：(n^{[1]},1)$时候  矩阵乘法很容易理解$z^{[1]} = w^{[1]}x + b^{1}$  $z^{[2]} = w^{[2]}a^{[1]} + b^{2}$</p><p>此时$a^{[2]} = g(z^{[1]})$   同理将多个数据样本平铺起来的时候变成平铺的向量</p><p>变成如下图</p><p><img src="https://img-blog.csdnimg.cn/20200407170144205.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>吴恩达深度学习课程第二课</title>
      <link href="/2023/10/11/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%8C%E8%AF%BE/"/>
      <url>/2023/10/11/%E5%90%B4%E6%81%A9%E8%BE%BE%E8%AF%BE%E7%A8%8B%E7%AC%AC%E4%BA%8C%E8%AF%BE/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="什么是方差和偏差？"><a href="#什么是方差和偏差？" class="headerlink" title="什么是方差和偏差？"></a>什么是方差和偏差？</h1><p>方差：如果一个模型的方差较高，意味着模型对于训练数据的小变化非常敏感，可能会导致过拟合（Overfitting）的问题，</p><p>偏差：偏差衡量了模型的预测值与实际观测值之间的差异或偏移。具体来说，它表示了模型在训练数据上的平均预测误差。如果一个模型的偏差较高，意味着模型在训练数据上不能很好地拟合，可能会导致欠拟合（Underfitting）的问题</p><p><strong>欠拟合就是没把大头当回事，过拟合就是将偶然太当真</strong></p><p>当涉及到模型的方差和偏差时，可以使用具体的例子来更好地理解这两个概念：</p><p>假设你正在开发一个简单的线性回归模型，用于预测房屋价格。你有一个包含房屋价格和各种特征（例如房屋大小、卧室数量、地理位置等）的数据集。以下是对方差和偏差的具体解释：</p><ol><li>方差：<ul><li>假设你多次随机划分你的数据集为训练集和测试集，然后训练相同的线性回归模型。如果不同训练集上训练的模型对于相同测试集产生的预测结果差异很大，那么模型的方差较高。这可能表示模型对于训练数据的小变化非常敏感。</li><li>例如，如果在一次模型训练中，模型预测某个房屋价值为$300,000，而在另一次训练中，相同模型对同一房屋的预测结果是$350,000，那么模型的方差较高。</li></ul></li><li>偏差：<ul><li>偏差表示模型的预测值与实际观测值之间的平均差异。如果你的线性回归模型对于训练数据集中的大多数房屋都产生了相对较大的预测误差，那么模型的偏差较高。</li><li>例如，如果线性回归模型对于所有房屋都低估了实际价格，那么它具有较高的偏差。</li></ul></li></ol><p>在实际机器学习应用中，你希望找到一个平衡点，使模型的方差和偏差都保持在适度水平。这可能需要尝试不同的模型复杂度（例如，多项式回归的不同阶数）、数据集大小、正则化技术等，以确保你的模型能够在训练数据和未见过的数据上都表现良好。这个平衡点有助于防止过拟合和欠拟合，从而获得更好的泛化性能。</p><h1 id="如何解决？"><a href="#如何解决？" class="headerlink" title="如何解决？"></a>如何解决？</h1><p>偏差高：欠拟合了，增加模型复杂度，先提升训练集上的性能，</p><p>方差高：扩充数据集、正则化、或者其他模型结构来解决高方差</p><h2 id="正则化："><a href="#正则化：" class="headerlink" title="正则化："></a>正则化：</h2><p>意思就是在成本函数J找到最小值  但是i成本函数更改了一点 加上了omit  $L_{2}$ regulazation 函数</p><p>此方法称为 L 2 正则化。因为这里用了欧几里德法线，被称为向量参数w 的 L 2 范数。<br><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230923114941305.png" alt="image-20230923114941305"></p><h3 id="为什么正则化可以减少过度拟合？"><a href="#为什么正则化可以减少过度拟合？" class="headerlink" title="为什么正则化可以减少过度拟合？"></a>为什么正则化可以减少过度拟合？</h3><ul><li>L1正则化通常用于特征选择和稀疏建模，因为它倾向于将某些参数压缩到零，从而实现了特征选择的效果。</li><li>L2正则化有助于改善模型的稳定性和泛化能力，但不会将参数压缩到零，因此它不太适用于特征选择，而更适合用于降低模型的复杂性。</li><li></li></ul><p><strong>复杂性惩罚</strong>：正则化向模型的损失函数添加一个正则化项，该项惩罚复杂模型。正则化项的大小由正则化强度参数（通常表示为λ）控制。</p><p><strong>特征选择</strong>：在L1正则化等一些技术中，正则化会导致某些模型参数变为零，从而实现了特征选择。这意味着模型忽略了一些不重要的特征，只保留了与任务相关的特征。</p><p><strong>降低参数之间的相关性</strong>：L2正则化等技术可以减小参数的幅度，从而降低了参数之间的相关性。相关性较高的参数可能导致模型对输入数据中的小变化过于敏感。</p><p><strong>泛化性能</strong>：正则化的主要目标是改善模型在未见过的数据上的性能，即提高模型的泛化性能。通过控制模型的复杂性，正则化可以帮助模型更好地适应新数据，而不仅仅是训练数据。</p><h4 id="为什么L1会导致某些w参数越来越小-？"><a href="#为什么L1会导致某些w参数越来越小-？" class="headerlink" title="为什么L1会导致某些w参数越来越小 ？"></a>为什么L1会导致某些w参数越来越小 ？</h4><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230923161950227.png" alt="image-20230923161950227"></p><p>求出$dw^{[l]}$后进行$w^{[l]}$更新时候 会导致</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230923162031753.png" alt="image-20230923162031753"></p><p>  $w^{[l]}$时刻变小</p><h1 id="Dropout正则化"><a href="#Dropout正则化" class="headerlink" title="Dropout正则化"></a>Dropout正则化</h1><p>意思就是i随机失活神经网络层的某层的各个节点 ，以便解决过度拟合的问题。</p><h3 id="如何实施？"><a href="#如何实施？" class="headerlink" title="如何实施？"></a>如何实施？</h3><p>常用的</p><p>即inverted dropout（反向随机失活），出于完整性考虑，我们用一个三层（ l = 3  ）网络来举例说明。编码中会有很多涉及到3的地方。我只举例说明如何在某一层中实施dropout。</p><h5 id="1-首先要定义向量-d-，-d-3-表示一个三层的dropout向量："><a href="#1-首先要定义向量-d-，-d-3-表示一个三层的dropout向量：" class="headerlink" title="1.首先要定义向量 d ， $d^{[3]}$  表示一个三层的dropout向量："></a>1.首先要定义向量 d ， $d^{[3]}$  表示一个三层的dropout向量：</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">d3=np.random.rand(a3.shape[0],a3.shape[1])</span><br></pre></td></tr></table></figure><p>然后看它是否小于某数，我们称之为keep-prob，keep-prob是一个具体数字，上个示例中它是0.5，而本例中它是0.8，它表示保留某个隐藏单元的概率，此处keep-prob等于0.8，它意味着消除任意一个隐藏单元的概率是0.2，它的作用就是生成随机矩阵，如果对 $ a^{[3]}$进行因子分解，效果也是一样的。$d^{[3]}$  是一个矩阵，每个样本和每个隐藏单元，其中$ d^{[3]}$</p><h5 id="2接下来就是将获取的激活函数与得到的d3矩阵乘一下-去掉某些节点"><a href="#2接下来就是将获取的激活函数与得到的d3矩阵乘一下-去掉某些节点" class="headerlink" title="2接下来就是将获取的激活函数与得到的d3矩阵乘一下 去掉某些节点"></a>2接下来就是将获取的激活函数与得到的d3矩阵乘一下 去掉某些节点</h5><p>接下来要做的就是从第三层中获取激活函数，这里我们叫它 $ a^{[3]} $ , $  a^{[3]} $  含有要计算的激活函数，  $  a^{[3]} $<br>  等于上面的 $ a^{[3]} $  乘以  $  d^{[3]} $  ，a3 =np.multiply(a3,d3)，这里是元素相乘，也可写为  $ a^{[3]}*=d^{[3]} $<br>  ，它的作用就是让  $  d^{[3]} $ 中所有等于0的元素（输出），而各个元素等于0的概率只有20%，乘法运算最终把 $  d^{[3]} $<br>中相应元素输出，即让  $  d^{[3]} $ 中0元素与  $  a^{[3]} $ 中相对元素归零。</p><p><img src="https://img-blog.csdnimg.cn/20200408165715591.png#pic_center" alt="img"></p><p>如果用python实现该算法的话，$ d^{[3]}$  则是一个布尔型数组，值为true和false，而不是1和0，乘法运算依然有效，python会把true和false翻译为1和0，</p><h5 id="3最后我们向外扩展-a-3-用它初一0-8，或者说是除以keep-prob的参数"><a href="#3最后我们向外扩展-a-3-用它初一0-8，或者说是除以keep-prob的参数" class="headerlink" title="3最后我们向外扩展$ a^{[3]}$,用它初一0.8，或者说是除以keep-prob的参数"></a>3最后我们向外扩展$ a^{[3]}$,用它初一0.8，或者说是除以keep-prob的参数</h5><p>为什么除以这个参数  意思就是最后我们得到$z^{[4]}$的时候 由于$z^4 = w^{[4]} * a^{[3]} + b^{[4]}$所以 $a^{[3]}$中有些元素被归零影响得到$z^{[4]}$的值</p><p>所以要除以keep-prob参数 弥补那损失的值</p><p>总结：Dropout方法参数尽量处在0.9左右 因为大量的失活可能会导致一些 </p><h2 id="其他正则化方法："><a href="#其他正则化方法：" class="headerlink" title="其他正则化方法："></a>其他正则化方法：</h2><p>一.数据扩增</p><p>就是将一组数据变形进而加入训练集，从而增加训练集。</p><p>二.<strong>early stopping</strong></p><p>就是训练代价函数J 的时候优化到一定的值后就停止优化。</p><p><strong>early stopping</strong>代表提早停止训练神经网络，但是你并不知道什么时候停止。</p><h1 id="归一化输入"><a href="#归一化输入" class="headerlink" title="归一化输入"></a>归一化输入</h1><p>第一步是零均值化。 </p><p>第二步是归一化方差</p><p><img src="https://img-blog.csdnimg.cn/20200408195806849.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>意思就是要让数据在一个范围内，不然处理的时候可能会有偏差。不然做梯度下降的时候可能会很慢。</p><p><img src="https://img-blog.csdnimg.cn/20200408195926132.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p><h3 id="梯度消失和梯度爆炸"><a href="#梯度消失和梯度爆炸" class="headerlink" title="梯度消失和梯度爆炸"></a>梯度消失和梯度爆炸</h3><p>意思就是在进行梯度计算的时候梯度优化w的时候可能会进行指数增长或者指数下降，层数躲起来的时候可能会对神经网络</p><p>计算产生影响。</p><p>下面是梯度爆炸和消失 就是 1.5的n次方和0.5的n次方一个接近无穷一个接近0 这就会导致梯度爆炸和消失</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230923212606523.png" alt="image-20230923212606523"></p><h3 id="神经网络的权重初始化"><a href="#神经网络的权重初始化" class="headerlink" title="神经网络的权重初始化"></a>神经网络的权重初始化</h3><p>意思就是初始化W根据不同的激活函数得到不同的权重初始化值</p><ul><li>权重$W^{[l]}$应该随机初始化以打破对称性。</li><li>将偏差$b^{[l]}$初始化为零是可以的。只要随机初始化了$W^{[l]}$，对称性仍然会破坏。</li></ul><h4 id="梯度检验1-13-梯度检验-深度学习第二课《改善深层神经网络》-Stanford吴恩达教授-Zhao-Jichao的博客-CSDN博客"><a href="#梯度检验1-13-梯度检验-深度学习第二课《改善深层神经网络》-Stanford吴恩达教授-Zhao-Jichao的博客-CSDN博客" class="headerlink" title="梯度检验1.13 梯度检验-深度学习第二课《改善深层神经网络》-Stanford吴恩达教授_Zhao-Jichao的博客-CSDN博客"></a>梯度检验<a href="https://blog.csdn.net/weixin_36815313/article/details/105399140">1.13 梯度检验-深度学习第二课《改善深层神经网络》-Stanford吴恩达教授_Zhao-Jichao的博客-CSDN博客</a></h4><ul><li>梯度检验可验证反向传播的梯度与梯度的数值近似值之间的接近度（使用正向传播进行计算）。</li><li>梯度检验很慢，因此我们不会在每次训练中都运行它。通常，你仅需确保其代码正确即可运行它，然后将其关闭并将backprop用于实际的学习过程。</li></ul><p>[TOC]</p><h1 id="mini-batch-梯度下降法"><a href="#mini-batch-梯度下降法" class="headerlink" title="mini-batch 梯度下降法"></a>mini-batch 梯度下降法</h1><p>意思就是向量化能够让你有效地对所有 m mm 个样本进行计算，允许你处理整个训练集，而无需某个明确的公式。所以我们要把训练样本放大巨大的矩阵 X XX 当中去，但是向量如果变得多了比如500 万个或者是更大的数，在对整个训练集执行梯度下降法时候，一个个的去正向传播会很慢。</p><p>所以如下图引入了一个子集 就是每个子集上面存放1000个X向量 ，在些子集被称为<strong>mini</strong>-<strong>batch</strong> 可以分成5000个 这个时候就得5000个<strong>mini-batch</strong> </p><p><img src="https://img-blog.csdnimg.cn/20200409142901171.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>同理对Y也进行相同的处理，也要相应地拆分 Y  的训练集。Y和X一样分成5000个<strong>mini-batch</strong>集合。</p><p>然后执行向前传播和梯度下降</p><p><img src="https://img-blog.csdnimg.cn/20200410102601470.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926130913272.png" alt="image-20230926130913272"></p><h2 id="之前我们学习的神将网络梯度下降叫batch下降法"><a href="#之前我们学习的神将网络梯度下降叫batch下降法" class="headerlink" title="之前我们学习的神将网络梯度下降叫batch下降法"></a>之前我们学习的神将网络梯度下降叫batch下降法</h2><h3 id="batch下降法和mini-batch下降法有什么不一样吗？"><a href="#batch下降法和mini-batch下降法有什么不一样吗？" class="headerlink" title="batch下降法和mini-batch下降法有什么不一样吗？"></a>batch下降法和mini-batch下降法有什么不一样吗？</h3><p><strong>batch</strong>下降法每次都需要对整个数据集进行遍历，每次迭代都 需要对整个数据集进行前向传播和反向传播，计算出全局梯度。通常用于小型计算集</p><p><strong>Mini-Batch </strong>下降法 可以将数据分成小批量，并且每次计算梯度，只是针对每次的小批量来进行计算，更高效。如图</p><p><img src="https://img-blog.csdnimg.cn/202004101101026.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>绿色线条为minibatch下降  紫色为随机梯度下降  蓝色为batch梯度下降</p><p>绿色更少更快</p><h1 id="指数加权平均"><a href="#指数加权平均" class="headerlink" title="指数加权平均"></a>指数加权平均</h1><p>什么是指数加权呢？ 吴恩达视频中用了一个测量美国温度的例子来举例说明这个问题。让温度函数设置为$v_t = 0.9v_(t-1) + 0.1  \theta_(t)$ 类似如下</p><p><img src="https://img-blog.csdnimg.cn/20200410111115746.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>然后通过β的值来挑取天数</p><p><img src="https://img-blog.csdnimg.cn/20200410111140546.png" alt="img"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926152736038.png" alt="image-20230926152736038"></p><p><img src="https://img-blog.csdnimg.cn/20200410111151384.png" alt="img"></p><p>就这样通过不同的数值来选取不同的天数从得到温度曲线</p><p>β=0.5   得到平均了2天的温度  运行得到黄线</p><p><img src="https://img-blog.csdnimg.cn/20200410111213618.png" alt="img"></p><h2 id="如何理解加权平均数"><a href="#如何理解加权平均数" class="headerlink" title="如何理解加权平均数"></a>如何理解加权平均数</h2><p>关键方程 $v_t = 0.9v_(t-1) + 0.1  \theta_(t)$</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926161405496.png" alt="image-20230926161405496"></p><p>所以这是一个加和并平均，100号数据，也就是当日温度。我们分析 $v_{100}$</p><p>  的组成，也就是在一年第100天计算的数据，但是这个是总和，包括100号数据，99号数据，97号数据等等。画图的一个办法是，假设我们有一些日期的温度，所以这是数据，这是 t ，所以100号数据有个数值，99号数据有个数值，98号数据等等， t 为100，99，98等等，这就是数日的温度数值。</p><p><img src="https://img-blog.csdnimg.cn/20200410112315838.png" alt="img"></p><p>然后构建指数衰减函数从0.1开始到 0.1 ∗ 0.9 0.1<em>0.90.1∗0.9 ，到 0.1 ∗ 0. 9 ,0.1</em> * $0.9^2$   ，以此类推，所以就有了这个指数衰减函数。</p><p><img src="https://img-blog.csdnimg.cn/20200410112326335.png" alt="img"></p><p>计算 $ v_{100}$ 是通过，把两个函数对应的元素，然后求和，用这个数值100号数据值乘以0.1，99号数据值乘以0.1乘以 $0.9^2$<br> ，这是第二项，以此类推，所以选取的是每日温度，将其与指数衰减函数相乘，然后求和，就得到了 $ v_{100}$</p><p><img src="https://img-blog.csdnimg.cn/2020041011233963.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926162348778.png" alt="image-20230926162348778"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926162446543.png" alt="image-20230926162446543"></p><p>​     总结：就是通过β的值不同可以预测不同曲线的温度</p><h2 id="偏差修正"><a href="#偏差修正" class="headerlink" title="偏差修正"></a>偏差修正</h2><p><img src="https://img-blog.csdnimg.cn/20200410115305389.png" alt="img"></p><p>红色曲线对应β值为0.9  绿色曲线对应β值为0.98  。</p><p>但实际在β=0、98的时候得到的并不是绿色曲线，而是紫色曲线，你可以注意到紫色曲线的起点较低，我们来看看怎么处理。</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926164708843.png" alt="image-20230926164708843"></p><p>总结：实用加权平均数偏差修正可以更好的减小误差。达到更精准的数据</p><h2 id="动量梯度下降法"><a href="#动量梯度下降法" class="headerlink" title="动量梯度下降法"></a>动量梯度下降法</h2><p>还有一种算法叫做<strong>Momentum</strong>，或者叫做动量梯度下降法，运行速度几乎总是快于标准的梯度下降算法，简而言之，基本的想法就是计算梯度的指数加权平均数，并利用该梯度更新你的权重，在本视频中，我们呢要一起拆解单句描述，看看你到底如何计算。</p><p><img src="https://img-blog.csdnimg.cn/20200410121556353.png" alt="img"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926171642685.png" alt="image-20230926171642685"></p><p>总结：意思就是在梯度下降的过程中，上下摆动的幅度变得很小，更新参数变得很慢很慢 ，从而需要给一点动量加速梯度下降的过程。反正是</p><p>减少纵轴方向的学习速度 加快横轴学习速度基于这个理念引入</p><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p><img src="https://img-blog.csdnimg.cn/20200410141914493.png" alt="img"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926174543211.png" alt="image-20230926174543211"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926174533314.png" alt="image-20230926174533314"></p><p>在横轴也就是w方向 我们希望学习速度快，而在垂直方向，也就是b方向，我们喜欢减缓摆动，所以会希望$S_{dw}$相对较小，除以一个较小的数，从而使得W更新后变得不是很小达到加速效果  并且此时希望b方向变得较较小，那么就希望$S_{db}$尽量大，使得b更新减去后面的数的时候， 让b更变得尽量小。从而达到目的</p><h2 id="Adam优化算法"><a href="#Adam优化算法" class="headerlink" title="Adam优化算法"></a>Adam优化算法</h2><p>Adam优化算法基本上就是将<strong>Momentum</strong>和<strong>RMSprop</strong>结合在一起，（Monmentum是动量梯度下降的意思<img src="https://img-blog.csdnimg.cn/20200410144947463.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img">)</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926200614510.png" alt="image-20230926200614510"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926200627460.png" alt="image-20230926200627460"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926200646914.png" alt="image-20230926200646914"></p><p>其中<img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926200709483.png" alt="image-20230926200709483"></p><p>是个缺省值 可要可不要</p><h2 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h2><p><strong>有指数衰减和mini-batch梯度下降法</strong></p><p>假设你要使用mini-batch梯度下降法，mini-batch数量不大，大概64或者128个样本，在迭代过程中会有噪音（蓝色线），下降朝向这里的最小值，但是不会精确地收敛，所以你的算法最后在附近摆动，并不会真正收敛，因为你用的 α \alphaα 是固定值，不同的mini-batch中有噪音。<br><img src="https://img-blog.csdnimg.cn/20200410151835648.png" alt="img"></p><p>但要慢慢减少学习率 α \alphaα 的话，在初期的时候， α \alphaα 学习率还较大，你的学习还是相对较快，但随着 α \alphaα 变小，你的步伐也会变慢变小，所以最后你的曲线（绿色线）会在最小值附近的一小块区域里摆动，而不是在训练过程中，大幅度在最小值附近摆动。</p><p>所以慢慢减少 α \alphaα 的本质在于，在学习初期，你能承受较大的步伐，但当开始收敛的时候，小一些的学习率能让你步伐小一些。</p><p>你可以这样做到学习率衰减，记得一代要遍历一次数据，如果你有以下这样的训练集，</p><p><img src="https://img-blog.csdnimg.cn/20200410151924257.png" alt="img"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230926201509039.png" alt="image-20230926201509039"></p><p>总结：学习率衰减是加快算法的方法</p><p>什么是超参数？  很简单如cnn中的学习率α或者是<strong>Momentum</strong>（动量梯度下降法）的参数 β  </p><h2 id="超参数的调试"><a href="#超参数的调试" class="headerlink" title="超参数的调试"></a>超参数的调试</h2><p>调试好超参数的范围</p><p><a href="https://blog.csdn.net/weixin_36815313/article/details/105447327">3.2 为超参数选择合适的范围-深度学习第二课《改善深层神经网络》-Stanford吴恩达教授_超学习为超参数选择合适的范围,比如选择神经网络的层数或者确定指数加权平均值中-CSDN博客</a></p><h2 id="超参数的实践-PandasVS-caviar"><a href="#超参数的实践-PandasVS-caviar" class="headerlink" title="超参数的实践  PandasVS caviar"></a>超参数的实践  PandasVS caviar</h2><p>  熊猫方法  vs甩籽方法</p><p>左边是一次只训练一个模型，所以很慢</p><p>右边是同时训练多个模型，对比梯度曲线获得更好的参数 如下图</p><h2 id=""><a href="#" class="headerlink" title=""></a><img src="https://img-blog.csdnimg.cn/20200411110602213.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></h2><h2 id="正则化网络的激活函数"><a href="#正则化网络的激活函数" class="headerlink" title="正则化网络的激活函数"></a>正则化网络的激活函数</h2><h3 id="1先归一化-2在激活函数"><a href="#1先归一化-2在激活函数" class="headerlink" title="1先归一化  2在激活函数"></a>1先归一化  2在激活函数</h3><p>归一化是加快训练速度</p><p>就是得到想要的Z的过程</p><p>普通神经网络得到的Z经过γ还有β的参数加入转换成$\hat{Z}$ 如下图</p><h2 id="-1"><a href="#-1" class="headerlink" title=""></a><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230928113840176.png" alt="image-20230928113840176"></h2><p><img src="https://img-blog.csdnimg.cn/20200411110936247.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl8zNjgxNTMxMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p> $\hat{Z^{[i]}}$代表的是某一层的第几个隐藏单元，若有表示第几次的隐藏单元可以这样写</p><p><img src="https://img-blog.csdnimg.cn/20200411110945828.png" alt="img"></p><p>$\hat{Z}$其实就是下一层的输入 如下</p><h2 id="-2"><a href="#-2" class="headerlink" title=" "></a> </h2><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20230928111417196.png" alt="image-20230928111417196"></p><h2 id="batch-norm-拟合神经网络"><a href="#batch-norm-拟合神经网络" class="headerlink" title="batch norm 拟合神经网络"></a>batch norm 拟合神经网络</h2><p>Batch归一化的关键步骤之一是在每个批次中计算特征的平均值，然后将这个平均值减去每个样本的对应特征值，以实现特征的归一化。这有助于确保每个批次中的特征在相似的尺度上，从而提高了深度神经网络的训练稳定性和速度。</p><p>由于计算$\hat{Z}$由于需要   先计算特征的平均值，然后将这个平均值减去每个样本的对应特征值。也就是需要+b然后-b 计算之间的差值</p><p>从而达到标准化的目的。所以我们可以不需要b的值（b值就是$b^{[i]}$）</p><h4 id="batch-norm函数会使我们的优化函数变得更容易优化"><a href="#batch-norm函数会使我们的优化函数变得更容易优化" class="headerlink" title="batch norm函数会使我们的优化函数变得更容易优化"></a>batch norm函数会使我们的优化函数变得更容易优化</h4><p><strong>Batch</strong>归一化将你的数据以<strong>mini-batch</strong>的形式逐一处理，但在测试时，你可能需要对每个样本逐一处理，我们来看一下怎样调整你的网络来做到这一点。</p><p><img src="https://img-blog.csdnimg.cn/20200411150615508.png" alt="img"></p><p>在训练时，这些就是用来执行Batch归一化的等式。在一个mini-batch中，你将mini-batch的 $z^{(i)}$ 值求和，计算均值，所以这里你只把一个mini-batch中的样本都加起来，我用 m mm 来表示这个mini-batch中的样本数量，而不是整个训练集。然后计算方差，再算 $ z^{(i)}_{norm}$ ，即用均值和标准差来调整，加上 $ \epsilon$ 是为了数值稳定性。 $ \tilde{z} $  是用 $\gamma$和 $ \beta$再次调整 $ z_{norm}$  得到的。</p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20231002164153910.png" alt="image-20231002164153910"></p><p><img src="https://img-blog.csdnimg.cn/20200411150639424.png" alt="img"></p><p><img src="C:\Users\阿千\AppData\Roaming\Typora\typora-user-images\image-20231002164218036.png" alt="image-20231002164218036"></p><h2 id="SOFTMAX回归"><a href="#SOFTMAX回归" class="headerlink" title="SOFTMAX回归"></a>SOFTMAX回归</h2><p>二分分类叫logic回归 那么多种分类叫做 softmax回归，意思就是分类的选项更多。<strong>就是分更多类</strong></p><p>具体计算如下</p><p><img src="https://img-blog.csdnimg.cn/20200411152940263.png" alt="img"></p><p>通常就是神经网络计算到z后在用softmax激活函数得到$\hat z$ 然后在得到概率</p><p>但是假如分两类就会回到logic回归上了</p><h1 id="训练一个Softmax分类器"><a href="#训练一个Softmax分类器" class="headerlink" title="训练一个Softmax分类器"></a>训练一个Softmax分类器</h1><p>代码实践</p><p><a href="https://blog.csdn.net/weixin_36815313/article/details/105455056">3.9 训练一个 Softmax 分类器-深度学习第二课《改善深层神经网络》-Stanford吴恩达教授_trainable soft-max classifier-CSDN博客</a></p><h2 id="深度学习框架tensorflow"><a href="#深度学习框架tensorflow" class="headerlink" title="深度学习框架tensorflow"></a>深度学习框架tensorflow</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import math</span><br><span class="line">import numpy as np</span><br><span class="line">import h5py</span><br><span class="line">import matplotlib.pyplot as plt</span><br><span class="line">import tensorflow as tf</span><br><span class="line">from tensorflow.python.framework import ops</span><br><span class="line">from tf_utils import load_dataset, random_mini_batches, convert_to_one_hot, predict</span><br><span class="line">import tensorflow.compat.v1 as tf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line">np.random.seed(1)</span><br><span class="line">tf.disable_v2_behavior()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">coefficients = np.array([[1.],[-20.],[100.]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">w = tf.Variable(0,dtype = tf.float32)</span><br><span class="line">x = tf.placeholder(tf.float32,[3,1])</span><br><span class="line"># cost = tf.add(tf.add(w**2,tf.multiply(-10.,w)),25)</span><br><span class="line">cost = x[0][0]*w**2 +x[1][0]*w + x[2][0]</span><br><span class="line">train =tf.train.GradientDescentOptimizer(0.01).minimize(cost)</span><br><span class="line"></span><br><span class="line">init = tf.global_variables_initializer()</span><br><span class="line">session = tf.Session()</span><br><span class="line">session.run(init)</span><br><span class="line"></span><br><span class="line">session.run(train,feed_dict=&#123;x:coefficients&#125;)</span><br><span class="line">print(session.run(w))</span><br><span class="line"></span><br><span class="line">for i in range(1000):</span><br><span class="line">    session.run(train,feed_dict=&#123;x:coefficients&#125;)</span><br><span class="line">    </span><br><span class="line">print(session.run(w))</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 吴恩达深度学习基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 吴恩达深度学习基础 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>致学姐</title>
      <link href="/2023/05/16/%E5%82%BB%E9%80%BC%E5%8F%B6%E5%AD%90-1/"/>
      <url>/2023/05/16/%E5%82%BB%E9%80%BC%E5%8F%B6%E5%AD%90-1/</url>
      
        <content type="html"><![CDATA[<p>学姐只是静静的站着，就已经<br><img src="/images/pasted-6.png" alt="玖涯博客"></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
